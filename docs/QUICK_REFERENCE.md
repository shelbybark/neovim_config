# Quick Reference Card

## ğŸ¯ At a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CodeCompanion + Ollama + Tailscale Integration             â”‚
â”‚ Quick Reference Card                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âŒ¨ï¸ Keymaps

| Keymap | Action | Mode |
|--------|--------|------|
| `<leader>cll` | Chat with Ollama | Normal, Visual |
| `<leader>cc` | Chat with Claude Haiku | Normal, Visual |
| `<leader>cs` | Chat with Claude Sonnet | Normal, Visual |
| `<leader>co` | Chat with Claude Opus | Normal, Visual |
| `<leader>ca` | Show CodeCompanion actions | Normal, Visual |
| `<leader>cm` | Show current model | Normal |

## ğŸ”§ Setup Checklist

### On Ollama Server
- [ ] `sudo systemctl edit ollama` â†’ Add `Environment="OLLAMA_HOST=0.0.0.0:11434"`
- [ ] `sudo systemctl restart ollama`
- [ ] `ollama pull mistral` (or your preferred model)
- [ ] `tailscale ip -4` â†’ Note the IP (e.g., 100.123.45.67)

### On Other Machines
- [ ] Add to `~/.zshrc` (or `~/.bashrc`):
  ```bash
  export OLLAMA_ENDPOINT="http://100.123.45.67:11434"
  ```
- [ ] `source ~/.zshrc` (reload shell)
- [ ] `curl $OLLAMA_ENDPOINT/api/tags` (test connection)
- [ ] Start Neovim and press `<leader>cll`

## ğŸ§ª Quick Tests

```bash
# Test Ollama is running
curl http://localhost:11434/api/tags

# Test remote access
curl http://100.x.x.x:11434/api/tags

# Test Tailscale
tailscale status
ping 100.x.x.x

# List models
ollama list

# Pull a model
ollama pull mistral
```

## ğŸ“Š Model Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model        â”‚ Size â”‚ Speed â”‚ Quality â”‚ Best For     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ orca-mini    â”‚ 3B   â”‚ âš¡âš¡âš¡ â”‚ â­â­   â”‚ Quick answersâ”‚
â”‚ mistral      â”‚ 7B   â”‚ âš¡âš¡  â”‚ â­â­â­ â”‚ Coding       â”‚
â”‚ neural-chat  â”‚ 7B   â”‚ âš¡âš¡  â”‚ â­â­â­ â”‚ Chat         â”‚
â”‚ llama2       â”‚ 7B   â”‚ âš¡âš¡  â”‚ â­â­â­ â”‚ General      â”‚
â”‚ dolphin-mix  â”‚ 8x7B â”‚ âš¡   â”‚ â­â­â­â­â”‚ Complex      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Troubleshooting Quick Fixes

| Problem | Quick Fix |
|---------|-----------|
| Connection refused | `ps aux \| grep ollama` (check if running) |
| Model not found | `ollama pull mistral` |
| Can't reach remote | `ping 100.x.x.x` (check Tailscale) |
| Env var not working | `echo $OLLAMA_ENDPOINT` (verify it's set) |
| Slow responses | Try smaller model: `ollama pull orca-mini` |

## ğŸ“ Important Files

| File | Purpose |
|------|---------|
| `lua/shelbybark/plugins/codecompanion.lua` | Main config (modified) |
| `docs/OLLAMA_SETUP.md` | Full setup guide |
| `docs/TROUBLESHOOTING.md` | Detailed troubleshooting |
| `docs/ARCHITECTURE.md` | Network diagrams |
| `docs/IMPLEMENTATION_CHECKLIST.md` | Step-by-step checklist |

## ğŸŒ Network Setup

```
Machine A (Ollama Server)
â”œâ”€ Ollama: http://localhost:11434
â”œâ”€ Tailscale IP: 100.123.45.67
â””â”€ OLLAMA_HOST=0.0.0.0:11434

Machine B (Client)
â”œâ”€ OLLAMA_ENDPOINT=http://100.123.45.67:11434
â””â”€ Connects via Tailscale VPN

Machine C (Client)
â”œâ”€ OLLAMA_ENDPOINT=http://100.123.45.67:11434
â””â”€ Connects via Tailscale VPN
```

## ğŸ’¾ Environment Variable

```bash
# Add to ~/.zshrc, ~/.bashrc, or ~/.config/fish/config.fish
export OLLAMA_ENDPOINT=\"http://100.123.45.67:11434\"

# Then reload
source ~/.zshrc  # or ~/.bashrc
```

## ğŸš€ Usage Flow

```
1. Press <leader>cll
   â†“
2. CodeCompanion opens chat window
   â†“
3. Reads OLLAMA_ENDPOINT env var
   â†“
4. Connects to Ollama server
   â†“
5. Type message and press Enter
   â†“
6. Ollama generates response
   â†“
7. Response appears in Neovim
```

## ğŸ“ Help Commands

```bash
# Check Ollama status
sudo systemctl status ollama

# View Ollama logs
journalctl -u ollama -f

# List available models
ollama list

# Pull a model
ollama pull <model-name>

# Check Tailscale
tailscale status

# Find your Tailscale IP
tailscale ip -4

# Test connection
curl http://localhost:11434/api/tags
curl http://100.x.x.x:11434/api/tags
```

## âš¡ Performance Tips

1. **Use 7B models** for best balance (mistral, neural-chat)
2. **Avoid 13B+ models** on slow networks
3. **Monitor latency**: `ping 100.x.x.x` (should be < 50ms)
4. **Run on GPU** if available for faster inference
5. **Close other apps** to free up resources

## ğŸ” Security Checklist

- âœ… Ollama only accessible via Tailscale
- âœ… All traffic encrypted end-to-end
- âœ… Uses private Tailscale IPs (100.x.x.x)
- âœ… No exposure to public internet
- âœ… Firewall rules can further restrict access

## ğŸ“‹ Common Commands

```bash
# Start Ollama
ollama serve

# Or with systemd
sudo systemctl start ollama

# Pull a model
ollama pull mistral

# List models
ollama list

# Remove a model
ollama rm mistral

# Test connection
curl http://localhost:11434/api/tags | jq '.models[].name'

# Check Tailscale
tailscale status

# Restart Ollama
sudo systemctl restart ollama
```

## ğŸ“ Learning Resources

- Ollama: https://github.com/ollama/ollama
- Tailscale: https://tailscale.com/kb/
- CodeCompanion: https://github.com/olimorris/codecompanion.nvim
- Neovim: https://neovim.io/

## ğŸ“ Notes

- Default model: `mistral` (change in codecompanion.lua line 40)
- Default endpoint: `http://localhost:11434` (override with env var)
- Keymaps use `<leader>` (usually `\` or `,`)
- All documentation in `docs/` folder

---

**Print this card and keep it handy!**

**Last Updated**: 2026-02-05
